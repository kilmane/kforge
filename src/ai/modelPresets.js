// src/ai/modelPresets.js
// Suggestions only. Users can always type exact model IDs manually.

export const MODEL_PRESETS = {
  // ------------------------
  // OpenAI
  // ------------------------
  openai: [
    "gpt-5-mini",   // ğŸŸ¢ Sandbox (cheap/test)
    "gpt-4.1-mini"  // ğŸŸ  Main
  ],

  // ------------------------
  // Gemini
  // ------------------------
  gemini: [
    "gemini-2.5-flash-lite",  // ğŸŸ¢ Sandbox
    "gemini-2.5-flash",       // ğŸŸ¢ Sandbox
    "gemini-3-flash-preview", // ğŸŸ  Main
    "gemini-2.5-pro",         // ğŸŸ  Main
    "gemini-3-pro-preview"    // ğŸ”´ Heavy
  ],

  // ------------------------
  // Anthropic (Claude)
  // ------------------------
  claude: [
    "claude-haiku-4-5",  // ğŸŸ¢ Sandbox
    "claude-sonnet-4-5", // ğŸŸ  Main
    "claude-opus-4-5"    // ğŸ”´ Heavy
  ],

  // ------------------------
  // DeepSeek (OpenAI-compatible style)
  // ------------------------
  deepseek: [
    "deepseek-chat",     // ğŸŸ¢ Sandbox
    "deepseek-reasoner"  // ğŸŸ  Main
  ],

  // ------------------------
  // Groq (fast inference; pricing depends on your Groq account)
  // ------------------------
  groq: [
    "llama-3.1-8b-instant",    // ğŸŸ¢ Sandbox
    "llama-3.3-70b-versatile"  // ğŸŸ  Main
  ],

  // ------------------------
  // OpenRouter (manual-first; free models rotate/deprecate)
  // ------------------------
  openrouter: [
    "mistralai/devstral-2512:free",
    "qwen/qwen3-coder:free",
    "meta-llama/llama-3.3-70b-instruct:free",
    "xiaomi/mimo-v2-flash:free"
  ],

  // ------------------------
  // Hugging Face
  // NOTE: IDs are stable; availability depends on your HF endpoint/provider.
  // ------------------------
  huggingface: [
    "Qwen/Qwen2.5-Coder-1.5B-Instruct",           // ğŸŸ¢ Sandbox
    "Qwen/Qwen2.5-Coder-7B-Instruct",             // ğŸŸ  Main
    "deepseek-ai/deepseek-coder-6.7b-instruct",   // ğŸŸ  Main
    "codellama/CodeLlama-7b-Instruct-hf",         // ğŸŸ  Main
    "bigcode/starcoder2-15b-instruct-v0.1"        // ğŸ”´ Heavy
  ],

  // ------------------------
  // Custom endpoints
  // Unknown model IDs â†’ leave empty
  // ------------------------
  custom: [],

  // ------------------------
  // Ollama (local or remote)
  // NOTE: Names must match what the user's Ollama has pulled.
  // ------------------------
  ollama: [
    "qwen2.5-coder:1.5b",   // ğŸŸ¢ Sandbox
    "mistral:7b",           // ğŸŸ¢ Sandbox
    "llama3.1:8b",          // ğŸŸ  Main
    "qwen2.5-coder:7b",     // ğŸŸ  Main
    "deepseek-coder:6.7b",  // ğŸŸ  Main
    "codellama:13b"         // ğŸ”´ Heavy
  ],

  // ------------------------
  // LM Studio
  // Keep empty until we add â€œList modelsâ€ from /v1/models into â€œMy modelsâ€
  // ------------------------
  lmstudio: [],

  // ------------------------
  // Mock
  // ------------------------
  mock: ["mock-1"]
};
